{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing-the-datasets\" data-toc-modified-id=\"Preprocessing-the-datasets-1\">Preprocessing the datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#To-do\" data-toc-modified-id=\"To-do-1.1\">To do</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stopwords\" data-toc-modified-id=\"Stopwords-1.1.1\">Stopwords</a></span></li><li><span><a href=\"#Punctuation\" data-toc-modified-id=\"Punctuation-1.1.2\">Punctuation</a></span></li><li><span><a href=\"#Lematizer\" data-toc-modified-id=\"Lematizer-1.1.3\">Lematizer</a></span></li><li><span><a href=\"#The-3-commented-cells-below-compute-the-lemmatized-version-of-the-df--&gt;-takes-a-long-time-to-run\" data-toc-modified-id=\"The-3-commented-cells-below-compute-the-lemmatized-version-of-the-df-->-takes-a-long-time-to-run-1.1.4\">The 3 commented cells below compute the lemmatized version of the df -&gt; takes a long time to run</a></span></li></ul></li><li><span><a href=\"#The-part-where-we-compute-the-predictions-of-dummy-classifier-trained-on-the-preprocessed-datasets\" data-toc-modified-id=\"The-part-where-we-compute-the-predictions-of-dummy-classifier-trained-on-the-preprocessed-datasets-1.2\">The part where we compute the predictions of dummy classifier trained on the preprocessed datasets</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good read : https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import open_by_tweets\n",
    "from utils import *\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#if wordcloud not already installed, run the following command : !python -m pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#uncomment the comments below if the packages have not already been downloaded\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;user&gt; yay ! ! #lifecompleted . tweet / facebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; #1dnextalbumtitle : feel for you / roll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>workin hard or hardly workin rt &lt;user&gt; at hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;user&gt; i saw . i'll be replying in a bit .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this is were i belong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;user&gt; anddd to cheer #nationals2013 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>we send an invitation to shop on-line ! here y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>just woke up , finna go to church</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;user&gt; agreed ! 12 more days left tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>monet with katemelo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>like dammm &lt;user&gt; lexis u got a lot to say whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>grateful today for a dream fulfilled ! ! my he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;user&gt; at home affairs shall do it later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>barca bout to beat real madrid on saturday doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;user&gt; a lot of parts of asia . especially rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; i wasn't even sleeping . so shut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>i have the worlds best dad . &lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; ab jaeyay werna meeting khatam h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;user&gt; no one doubts that ability !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;user&gt; check my tweet pic out . that was the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>just got my mid-term and i'm impressed ! ! #happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>my summer days : 1 . ) work from 10:30- 2:30 i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;user&gt; lol nooo .. food is ur friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;user&gt; #16millionbritneyfan rt and tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;user&gt; but seriously though .. it's called van...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>&lt;user&gt; hey what's good take a listen #fb #brok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>&lt;user&gt; don't apologize . you are doing your be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>texting c h y n a , she got me laughing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>&lt;user&gt; now i'm following u white girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>woot ! friday is here it's heating up between ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; what did he do when you grab his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>&lt;user&gt; oh yeah i know what ones those are good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>as long as you dont make this awkward we shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>i just ousted anna j . as the mayor of anna's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>&lt;user&gt; kerp being all negative , see what happens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; pro-tip for fellas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>&lt;user&gt; jgn la takut . u mest dpt mumtaz punya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>gonna be headed home from this job in a half h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>shouts out to &lt;user&gt; for telling me how it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>&lt;user&gt; does it make you think of me dancing ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>i love knowing that i get to hang out with my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>&lt;user&gt; i'll enough fun for you too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>&lt;user&gt; i just heard you're out with &lt;user&gt; thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>&lt;user&gt; aww thanks dj tim ! xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>i may say \" i hate you . \" like a billion time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>&lt;user&gt; can i be apart of the thumbs club pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>linfield tickets go on sale 2morrow good deals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>&lt;user&gt; coming to visit you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>it really is a date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>&lt;user&gt; hey gina what's up ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>&lt;user&gt; sas 9.1 . 3 and 9.2 , east 5 , s-plus 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; um gord ... i just read your pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>&lt;user&gt; i'm so excited for tomorrow ! look out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>i always wondered what the job application is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100001 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet\n",
       "0       <user> i dunno justin read my mention or not ....\n",
       "1       because your logic is so dumb , i won't even c...\n",
       "2       \" <user> just put casper in a box ! \" looved t...\n",
       "3       <user> <user> thanks sir > > don't trip lil ma...\n",
       "4       visiting my brother tmr is the bestest birthda...\n",
       "5       <user> yay ! ! #lifecompleted . tweet / facebo...\n",
       "6       <user> #1dnextalbumtitle : feel for you / roll...\n",
       "7       workin hard or hardly workin rt <user> at hard...\n",
       "8              <user> i saw . i'll be replying in a bit .\n",
       "9                                   this is were i belong\n",
       "10                 <user> anddd to cheer #nationals2013 ?\n",
       "11      we send an invitation to shop on-line ! here y...\n",
       "12                      just woke up , finna go to church\n",
       "13                  <user> agreed ! 12 more days left tho\n",
       "14                                    monet with katemelo\n",
       "15      like dammm <user> lexis u got a lot to say whe...\n",
       "16      grateful today for a dream fulfilled ! ! my he...\n",
       "17               <user> at home affairs shall do it later\n",
       "18         barca bout to beat real madrid on saturday doe\n",
       "19      <user> a lot of parts of asia . especially rat...\n",
       "20      <user> <user> i wasn't even sleeping . so shut...\n",
       "21                        i have the worlds best dad . <3\n",
       "22      <user> <user> ab jaeyay werna meeting khatam h...\n",
       "23                    <user> no one doubts that ability !\n",
       "24      <user> check my tweet pic out . that was the o...\n",
       "25      just got my mid-term and i'm impressed ! ! #happy\n",
       "26      my summer days : 1 . ) work from 10:30- 2:30 i...\n",
       "27                   <user> lol nooo .. food is ur friend\n",
       "28               <user> #16millionbritneyfan rt and tweet\n",
       "29      <user> but seriously though .. it's called van...\n",
       "...                                                   ...\n",
       "99971   <user> hey what's good take a listen #fb #brok...\n",
       "99972   <user> don't apologize . you are doing your be...\n",
       "99973             texting c h y n a , she got me laughing\n",
       "99974               <user> now i'm following u white girl\n",
       "99975   woot ! friday is here it's heating up between ...\n",
       "99976   <user> <user> what did he do when you grab his...\n",
       "99977   <user> oh yeah i know what ones those are good...\n",
       "99978   as long as you dont make this awkward we shoul...\n",
       "99979   i just ousted anna j . as the mayor of anna's ...\n",
       "99980   <user> kerp being all negative , see what happens\n",
       "99981   <user> <user> <user> <user> pro-tip for fellas...\n",
       "99982   <user> jgn la takut . u mest dpt mumtaz punya ...\n",
       "99983   gonna be headed home from this job in a half h...\n",
       "99984   shouts out to <user> for telling me how it is ...\n",
       "99985   <user> does it make you think of me dancing ar...\n",
       "99986   i love knowing that i get to hang out with my ...\n",
       "99987                  <user> i'll enough fun for you too\n",
       "99988   <user> i just heard you're out with <user> thi...\n",
       "99989                       <user> aww thanks dj tim ! xx\n",
       "99990   i may say \" i hate you . \" like a billion time...\n",
       "99991   <user> can i be apart of the thumbs club pleas...\n",
       "99992   linfield tickets go on sale 2morrow good deals...\n",
       "99993                          <user> coming to visit you\n",
       "99994                                 it really is a date\n",
       "99995                         <user> hey gina what's up ?\n",
       "99996   <user> sas 9.1 . 3 and 9.2 , east 5 , s-plus 8...\n",
       "99997   <user> <user> um gord ... i just read your pro...\n",
       "99998   <user> i'm so excited for tomorrow ! look out ...\n",
       "99999   i always wondered what the job application is ...\n",
       "100000                                                   \n",
       "\n",
       "[100001 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load tweets in list where each element is a tweet\n",
    "\n",
    "with open('data/train_pos.txt',\"r\") as file:\n",
    "    train_pos = file.read().split('\\n')\n",
    "#train_pos.remove('')\n",
    "\n",
    "train_pos = pd.DataFrame({'tweet' : train_pos}) #create the dataframe\n",
    " \n",
    "#train_pos = pd.read_csv('data/train_pos.txt', sep=\"\\n\", header=None)    \n",
    "    \n",
    "with open('data/train_neg.txt',\"r\") as file:\n",
    "    train_neg = file.read().split('\\n')\n",
    "#train_neg.remove('')\n",
    "train_neg = pd.DataFrame({'tweet' : train_neg})\n",
    "    \n",
    "#let's check it worked correctly\n",
    "train_pos\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From observing the tweets above, we can already think of different ideas for preprocessing (note that maybe some of them could make our model worse):\n",
    "- removing punctuation (maybe keep ! and ?)\n",
    "- correct spelling and slang word, ie., 'u','ur', 'nite',...\n",
    "- lower case everything\n",
    "- seperate hashtag in words (?) , ie., \"#iloveyou\" \n",
    "- removing repetitive letters inside word, ie, 'loooove' (seems complicated to do : https://stackoverflow.com/questions/20516100/term-split-by-hashtag-of-multiple-words but maybe we could try https://github.com/matchado/HashTagSplitter)\n",
    "- removing '&lt;url&gt;'\n",
    "- n-grams : some words should be taken as two words = one word\n",
    "- lemmatization and/or stemming (https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8)\n",
    "- emojis : 'XD', '<3', ':p',...\n",
    "- removing stopwords\n",
    "- removing the very frequent words that appear in both dataset\n",
    "- removing numbers ?\n",
    "- removing short words ?\n",
    "- transforming all variants of \"haha\" to \"haha\"\n",
    "\n",
    "However we can also note that :\n",
    "- '<3' is a supposed to be a heart so removing '<' with the punctuation will make us lose some important information\n",
    "- maybe some hashtags should stay a hashtag?\n",
    "- maybe '!' is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do \n",
    "**Implement everything in methods, we need a pipeline to process test data using the same steps as for the train data**\n",
    "* Remove blank tweets: Zeineb\n",
    "* Open contractions & transform @ into at or u into you, etc...: Zeineb\n",
    "* Separate punctuation from words like : 'word...' becomes 'word ...'\n",
    "* Separate alphabet & numbers (e.g. 398stomach)\n",
    "* removing repetitive letters inside word, ie, 'loooove' (seems complicated to do : https://stackoverflow.com/questions/20516100/term-split-by-hashtag-of-multiple-words but maybe we could try https://github.com/matchado/HashTagSplitter)\n",
    "* lemmatization and/or stemming (https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8): DONE\n",
    "* removing irrelevant punctuation (keep ! and ?, watch out for emojis, maybe remove isolated punctuation (there's a space before and after)) & when there are two exclamation points (for ex) we can transform them into a word (like doubleinterrogation): Lilia\n",
    "* stopwords, keep relevant ones: Lilia\n",
    "* transforming all variants of \"haha\" to \"haha\" : Zeineb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', 'as', 'for', 'when', 'this', 'couldn', 'under', \"weren't\", 't', 'should', 've', 'they', \"mightn't\", \"shan't\", 'both', 'further', \"aren't\", \"doesn't\", 'once', 'do', 'theirs', 'wasn', 'wouldn', 'user', 'are', 'few', 'ain', \"mustn't\", 'you', \"don't\", \"you've\", 'hasn', \"needn't\", \"shouldn't\", 'these', 'now', 'yourselves', 'had', 'by', 'has', 'or', 'o', 'did', 'more', 'again', 'been', 'at', 'where', 'nor', 'which', 'from', 'so', \"won't\", \"wasn't\", 'here', 'y', 'an', 'ourselves', 'it', 'between', 'in', 'being', 'while', 'those', 'whom', 'our', 'during', 'up', 'but', 'll', 'i', 'how', 'on', 'why', 'through', 'having', 'very', 'about', 'same', 'below', 'shouldn', \"hasn't\", 'other', 'url', 'because', 'with', 'over', \"isn't\", 'his', 'themselves', 'them', 'aren', 'her', 'a', 'don', 'won', 'herself', \"you're\", 'out', 'against', 'its', 'then', 'ours', 'above', \"hadn't\", 'after', 'there', 'hers', 'before', 'their', 'he', 'm', 'does', 'that', 'is', 'of', 'd', \"wouldn't\", 'needn', 'doing', 'all', 'doesn', 'didn', 'my', 'yours', 'too', 'isn', 'such', 'to', 'what', 'each', 'your', 'only', 'myself', 'can', \"it's\", 'we', 'own', 'hadn', 'shan', 'off', 'be', 'the', 'will', 'she', 'mustn', \"you'll\", \"haven't\", 'am', 'itself', 'and', 'have', 'most', 'some', 'just', \"that'll\", 'weren', 'down', 'yourself', 'mightn', 'ma', 'me', 's', \"you'd\", 'no', 'than', \"didn't\", 'was', 'himself', 'any', 'were', 're', \"couldn't\", \"she's\", 'until', 'not', \"should've\", 'if', 'haven', 'into', 'who'}\n"
     ]
    }
   ],
   "source": [
    "#list of stopword from nltk + \"url\" added -> maybe we should remove some of them, ie., 'but'\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords.add(\"url\")\n",
    "stopWords.add(\"user\")\n",
    "print(stopWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying the occurrence of stopwords in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alternate way \n",
    "# Create an empty dictionary \n",
    "d_pos = dict() \n",
    "# Loop through each line of the file \n",
    "for tweet in train_pos: \n",
    "    # Remove the leading spaces and newline character \n",
    "    tweet = tweet.strip() \n",
    "  \n",
    "    # Convert the characters in line to  \n",
    "    # lowercase to avoid case mismatch \n",
    "    tweet = tweet.lower() \n",
    "  \n",
    "    # Split the line into words \n",
    "    words = tweet.split(\" \") \n",
    "  \n",
    "    # Iterate over each word in line \n",
    "    for word in words: \n",
    "        # Check if the word is already in dictionary \n",
    "        if word in d_pos: \n",
    "            # Increment count of word by 1 \n",
    "            d_pos[word] = d_pos[word] + 1\n",
    "        else: \n",
    "            # Add the word to dictionary with count 1 \n",
    "            d_pos[word] = 1\n",
    "            \n",
    "#Print contents of dictionary\n",
    "for key in list(d_pos.keys()): \n",
    "    print(key, \":\", d_pos[key]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary \n",
    "d_neg = dict() \n",
    "# Loop through each line of the file \n",
    "for tweet in train_neg: \n",
    "    # Remove the leading spaces and newline character \n",
    "    tweet = tweet.strip() \n",
    "  \n",
    "    # Convert the characters in line to  \n",
    "    # lowercase to avoid case mismatch \n",
    "    tweet = tweet.lower() \n",
    "  \n",
    "    # Split the line into words \n",
    "    words = tweet.split(\" \") \n",
    "  \n",
    "    # Iterate over each word in line \n",
    "    for word in words: \n",
    "        # Check if the word is already in dictionary \n",
    "        if word in d_neg: \n",
    "            # Increment count of word by 1 \n",
    "            d_neg[word] = d_neg[word] + 1\n",
    "        else: \n",
    "            # Add the word to dictionary with count 1 \n",
    "            d_neg[word] = 1\n",
    "            \n",
    "#Print contents of dictionary\n",
    "for key in list(d_neg.keys()):\n",
    "    print(key, \":\", d_neg[key]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for stopword in stopWords:\n",
    "#     plt.figure(figsize=(3, 2))\n",
    "#     plt.title(\"Histogram for {}\".format(stopword))\n",
    "#     ax = sns.barplot(['pos', 'neg'],[count_vect_pos.vocabulary_.get(stopword), count_vect_neg.vocabulary_.get(stopword)], palette=['green', 'red'])\n",
    "\n",
    "for stopword in stopWords:\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.title(\"Histogram for {}\".format(stopword))\n",
    "    ax = sns.barplot(['pos', 'neg'],[d_pos.get(stopword), d_neg.get(stopword)], palette=['green', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tweet in train_neg:\n",
    "    if  ('-' in tweet):\n",
    "        print(tweet)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for punct in string.punctuation:\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.title(\"Histogram for {}\".format(punct))\n",
    "    ax = sns.barplot(['pos', 'neg'],[d_pos.get(punct), d_neg.get(punct)], palette=['green', 'red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep this punctuation\n",
    "len('!#&()-/:<>[]^|}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the lemmatizer to really work, we need to give a tag to indicate the nature \n",
    "#of the word (adv, adj, verb,noun,...) because if we don't, it treats every word\n",
    "#as a noun (ie, went stays went instead of go)\n",
    "\n",
    "#taken from : https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    #pos_tag doesn't have the same tag than wordnet therefore we have to modify the tag\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    lemmatize a given sentence by first giving a tag for each word and then lemmatizing\n",
    "    the word based on its tag\n",
    "     \n",
    "    Returns:\n",
    "    list:contains the lemmatized words of the parameter sentence\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks : \n",
    "- I notice that 'ing'-form of verbs are often 'in' and are therefore not classify as verb during lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVOID USING IT (WILL PROBABLY BE REMOVED)\n",
    "def preprocess_tweet(tweets, punctuation = string.punctuation, stopwords = stopWords):\n",
    "    \"\"\"\n",
    "    preprocessed the given list of tweets :\n",
    "        - lowercase\n",
    "        - remove punctation\n",
    "        - remove stopwords\n",
    "        - lemmatize\n",
    "         \n",
    "        \n",
    "    Returns:\n",
    "    list of the tokenized and preprocessed version of the tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    #convert every tweet to lowercase\n",
    "    tweets = [tweet.lower() for tweet in tweets]\n",
    "    \n",
    "    #remove punctuation\n",
    "    tweets = [tweet.translate(str.maketrans('', '', punctuation)) for tweet in tweets]\n",
    "    \n",
    "    tweets_mod = []\n",
    "    for tweet in tweets:\n",
    "        #lemmatize the sentence and give back a list of words\n",
    "        words = lemmatize_sentence(tweet) \n",
    "        \n",
    "        #remove the words that belong to stopwords (list)\n",
    "        words = [word for word in words if (word not in stopwords)]\n",
    "        \n",
    "        tweets_mod.append(words)\n",
    "        \n",
    "    return tweets_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_df(df, punctuation = string.punctuation, stopwords = stopWords, lemmatize = False):\n",
    "    \"\"\"\n",
    "    preprocessed the given list of tweets :\n",
    "        - lowercase\n",
    "        - remove punctation\n",
    "        - remove stopwords\n",
    "        - lemmatize the words if lemmatize argument set to True\n",
    "         \n",
    "        \n",
    "    Returns:\n",
    "    initial dataframe preprocessed\n",
    "    \"\"\"\n",
    "    \n",
    "    df['tweet'] = df['tweet'].apply(lambda x : x.lower()) #lowercase\n",
    "    df['tweet'] = df['tweet'].apply(lambda x : x.translate(str.maketrans('', '', punctuation))) #remove punctations\n",
    "    if(lemmatize):\n",
    "        df['tweet'] = df['tweet'].apply(lambda x : lemmatize_sentence(x))\n",
    "    else:\n",
    "        df['tweet'] = df['tweet'].apply(lambda x : x.split())\n",
    "        \n",
    "    df['tweet'] = df['tweet'].apply(lambda x: [word for word in x if (word not in stopWords)])#remove stopwords\n",
    "    df['tweet'] = df['tweet'].apply(lambda x : ' '.join(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_clean = preprocess_df(train_pos)\n",
    "train_neg_clean = preprocess_df(train_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 commented cells below compute the lemmatized version of the df -> takes a long time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos_lem = preprocess_df(train_pos, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_neg_lem = preprocess_df(train_neg, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos_lem.to_csv(\"train_pos_lem.csv\")\n",
    "#train_neg_lem.to_csv(\"train_neg_lem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "#preprocess_tweet([\"hEllo I went to the movies and ate chips!!!!!\", \"#12\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_words(tokenized_tweets):\n",
    "    \"\"\"\n",
    "    Plot a wordcloud of every word in all tweets\n",
    "    \"\"\"\n",
    "    #we need a string with all the words of all the tweets\n",
    "    all_words = [item for sublist in tokenized_tweets for item in sublist]#list of (list of words) -> list of (sentence)\n",
    "    all_words = ' '.join(word for word in all_words)\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud_words(train_pos_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud_words(train_neg_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The part where we compute the predictions of dummy classifier trained on the preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = merge_datasets_with_labels(train_pos_clean, train_neg_clean)\n",
    "#df_cleaned_lem = merge_datasets_with_labels(train_pos_lem, train_neg_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model's accuracy is 0.748345033099338\n"
     ]
    }
   ],
   "source": [
    "clf, count_vect = create_tfidf_clf(df_cleaned,ngram=(2,3)) #accuracy of preprocessed model without lemmatization\n",
    "#clf_lem, count_vect_lem = create_tfidf_clf(df_cleaned_lem,ngram=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model's accuracy is 0.7667446651066979\n"
     ]
    }
   ],
   "source": [
    "clf_1, count_vect_1 = create_tfidf_clf(df_cleaned,ngram=(1,1)) #accuracy of preprocessed model without lemmatization\n",
    "#clf_lem_1, count_vect_lem_1 = create_tfidf_clf(df_cleaned_lem,ngram=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unknown = test_dataset_to_df('data/test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sea doo pro sea scooter sports portable seadoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shucks well work week cant come cheer oh put b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant stay away bug thats baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maam lol im perfectly fine contagious anymore ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>whenever fall asleep watching tv always wake h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>needs get rid thing scares lol dont need car e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>whatever terrible mood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yesss rt thanks jordan love im gonna call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>friend text check last night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>followback please ur unitytour come europe sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>watch yall dumb asses get lock today happy420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>obsessed phasell killed best album ever love y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>robert de niro gay name like lewy id understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>canada grade 12 since dont grade 12 11th sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>please say hi denmark would amazing live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>finally home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3x3 custom picture frame poster frame 12 wide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>new followers mention followback boo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yep looks like best team stay proper form see bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nhls bettman suspension criticism gamesmanship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>way im pro andd lool rubbish also see undatabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>barrel pickles 22 pound german pickles crisp c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>completed whole cup tea today jessicas thats f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>expo precision point whiteboard eraser 8 473kf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>yougetmajorpointsif play soccer idc sport socc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>task essentials harness leather scribe keepers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>needtoreesthisshit shouldhavereaditwithaglasso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hahahaha ok ill come tons blankets cat hat soup 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lauries garden design protective decal skin st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>way hes mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>clear new goths learning dance youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>clear new goths learning dance youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>another day home sick already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>1 wee golden nuggets simmey quotesofwisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>disney movies classic films always</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>09x30 custom picture frame poster frame 126 wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>09x30 custom picture frame poster frame 126 wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>glad arrived safely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>im gonna care ur going either im going happy p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>monk season four dvd return scene crime emmy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>want wish beautiful cousin happy 19th birthday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>rt first rule sex rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>stay u darla rt know hahaha rt 4 u rani rt yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>embarrassing thing thats ever happened ahahaa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>wonder ever followback probably cause hates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>wonder ever followback probably cause hates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>anaheim ducks cake cupcake toppers 12 pack qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>shoutout go follow guys x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>good morning brother noon england hav great day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>youre welcome mrbruce tell kun rt said thank g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>follow back please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>omg screamed end thats amazing youre beautiful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>aww thats dissapointing cant make username pot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>thankyouu po</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>im afraid youre coming back lot rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>nice time w friend lastnite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>please stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>without daughter dvd twotime oscar r winner sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>fun class sweetcheeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>making r e l difference get r e l recreational...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "1      sea doo pro sea scooter sports portable seadoo...\n",
       "2      shucks well work week cant come cheer oh put b...\n",
       "3                          cant stay away bug thats baby\n",
       "4      maam lol im perfectly fine contagious anymore ...\n",
       "5      whenever fall asleep watching tv always wake h...\n",
       "6      needs get rid thing scares lol dont need car e...\n",
       "7                                 whatever terrible mood\n",
       "8        yesss rt thanks jordan love im gonna call later\n",
       "9                           friend text check last night\n",
       "10     followback please ur unitytour come europe sweden\n",
       "11         watch yall dumb asses get lock today happy420\n",
       "12     obsessed phasell killed best album ever love y...\n",
       "13     robert de niro gay name like lewy id understan...\n",
       "14        canada grade 12 since dont grade 12 11th sucks\n",
       "15              please say hi denmark would amazing live\n",
       "16                                          finally home\n",
       "17     3x3 custom picture frame poster frame 12 wide ...\n",
       "18                  new followers mention followback boo\n",
       "19     yep looks like best team stay proper form see bit\n",
       "20     nhls bettman suspension criticism gamesmanship...\n",
       "21     way im pro andd lool rubbish also see undatabl...\n",
       "22     barrel pickles 22 pound german pickles crisp c...\n",
       "23     completed whole cup tea today jessicas thats f...\n",
       "24     expo precision point whiteboard eraser 8 473kf...\n",
       "25     yougetmajorpointsif play soccer idc sport socc...\n",
       "26     task essentials harness leather scribe keepers...\n",
       "27     needtoreesthisshit shouldhavereaditwithaglasso...\n",
       "28     hahahaha ok ill come tons blankets cat hat soup 3\n",
       "29     lauries garden design protective decal skin st...\n",
       "30                                          way hes mine\n",
       "...                                                  ...\n",
       "9971              clear new goths learning dance youtube\n",
       "9972              clear new goths learning dance youtube\n",
       "9973                       another day home sick already\n",
       "9974          1 wee golden nuggets simmey quotesofwisdom\n",
       "9975                  disney movies classic films always\n",
       "9976   09x30 custom picture frame poster frame 126 wi...\n",
       "9977   09x30 custom picture frame poster frame 126 wi...\n",
       "9978                                 glad arrived safely\n",
       "9979   im gonna care ur going either im going happy p...\n",
       "9980   monk season four dvd return scene crime emmy a...\n",
       "9981   want wish beautiful cousin happy 19th birthday...\n",
       "9982                             rt first rule sex rules\n",
       "9983   stay u darla rt know hahaha rt 4 u rani rt yes...\n",
       "9984   embarrassing thing thats ever happened ahahaa ...\n",
       "9985         wonder ever followback probably cause hates\n",
       "9986         wonder ever followback probably cause hates\n",
       "9987   anaheim ducks cake cupcake toppers 12 pack qua...\n",
       "9988                           shoutout go follow guys x\n",
       "9989     good morning brother noon england hav great day\n",
       "9990   youre welcome mrbruce tell kun rt said thank g...\n",
       "9991                                  follow back please\n",
       "9992   omg screamed end thats amazing youre beautiful...\n",
       "9993   aww thats dissapointing cant make username pot...\n",
       "9994                                        thankyouu po\n",
       "9995                im afraid youre coming back lot rain\n",
       "9996                         nice time w friend lastnite\n",
       "9997                                         please stop\n",
       "9998   without daughter dvd twotime oscar r winner sa...\n",
       "9999                               fun class sweetcheeks\n",
       "10000  making r e l difference get r e l recreational...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unknown_clean = preprocess_df(df_unknown)#preprocessed the dataframe to predict\n",
    "df_unknown_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tfidf_clf(df_unknown_clean,clf_1,count_vect_1, \"dummy_without_lem1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_unknown_clean_lem = preprocess_df(df_unknown,lemmatize=True)\n",
    "#predict_tfidf_clf(df_unknown_clean_lem,clf_lem,count_vect_lem, \"dummy_with_lem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
